 \documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

%\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
%\lhead{MA4413 2013} \rhead{Mr. Kevin O'Brien}
%\chead{Midterm Assessment 1 }
%\input{tcilatex}

\begin{document}


%---------------------------------------------------------------- %
\newpage
\noindent {\Large \textbf{MA4413 Weeks 12 and 13 Tutorials}}

%------------------------------------------------------------ %
\subsection*{Question 1a}
Consider a source $X$ that produces five symbols with probabilities 1/2, 1/4, 1/8, 1/16 and 1/16. Determine the source entropy $H(x)$. 

\subsection*{Question 1b}

A input source is a random variable X with a four letter alphabet $\{A,B,C,D\}$.
There are four different probability distributions presented below. Compute the entropy for each case.
\begin{center}
\begin{tabular}{|c c|c|c|c|c|}
\hline	&	$X_i$	&	A	&	B	&	C	&	D	\\ \hline
Case 1	&	$p(X_i)$	&	0.25	&	0.25	&	0.25	&	0.25	\\ \hline
Case 2	&	$p(X_i)$	&	0.25	&	0.5	&	0.125	&	0.125	\\ \hline
Case 3	&	$p(X_i)$	&	0.7	&	0.1	&	0.1	&	0.1	\\ \hline
Case 4	&	$p(X_i)$	&	0.97	&	0.01	&	0.01	&	0.01	\\ \hline
\end{tabular} 
\end{center}
\subsection*{Question 1c}
Consider a source $X$ that produces 8 symbols with equal probabilities for each symbol. Determine the source entropy $H(x)$. 
%------------------------------------------------------------ %


\subsection*{Question 2}

The input source to a noisy communication channel is a random variable X over three symbols $\{a,b,c\}$. The output from this channel is a random variable $Y$ over the same three symbols. The joint distribution of the these two random variables is as follows:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
	&	x=a	&	x=b	&	x=c	\\ \hline
y=a	&	0.25	&	0	&	0.125	\\ \hline
y=b	&	0	&	0.125	&	0	\\ \hline
y=c	&	0.125	&	0.25	&	0.125	\\ \hline
\end{tabular} 
\end{center}

\begin{itemize}
\item Write down the marginal distributions for X and Y.

\item Compute the marginal entropies $H(X)$ and $H(Y)$

\item Compute the joint entropy $H(X,Y)$ of the two random variables.
\end{itemize}
\newpage
\subsection*{Question 3a}
A four letter alphabet is encoded into binary form according to
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline Case 1	&	A:  10 	&	C:  110	&	G:  111 &	T:  0  \\ \hline
Case 2	&	A:  00	&	C:  01	&		G: 10	&	T: 11	\\ \hline
\end{tabular} 
\end{center}
Using the code presented in case 1, decode the following sequence:
\begin{verbatim}	
11110001011010
\end{verbatim}

\noindent Encode this message using the code from case 2. Compare the length of messages in both cases.

\subsection*{Question 3b}
Given that the alphabet has the following distribution 
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x_i$	& A	& C	& G	& T \\ \hline
$p(x_i)$	& 0.25	& 0.125	& 0.125	& 0. 5 \\ \hline
\end{tabular} 
\end{center}
Compute the average symbol length for both cases.

%-----------------------------------------%
\subsection*{Question 4}
A DMS X has live symbols $\{x_l,x_2,x_3,x_4,x_5\}$ with $P(x_1) = 0.4$, $P(x_2)=0.19$, $P(x_3) =0.16$,
$P(x_4) = 0.15$, and $P(x_5) = 0.1$.
\begin{itemize}
\item[(a)] Construct the Shannon-Fano code for X, and calculate the efficiency of the code.
\item[(b)] Repeat for the Huffman code and compare the results.
\end{itemize}

\subsection*{Question 5}

A discrete memoryless source has a five symbol alphabet $\{x_1,x_2,x_3,x_4,x_5\}$ with the following probabilities 0.2,0.15,0.05,0.10 and 0.5.

\begin{itemize}
\item[(i)] Construct a Shannon-Fano code for $X$, and calculate the code efficiency.
\item[(ii)] Construct a Huffman code for $X$, and calculate the code efficiency.
\end{itemize}

%------------------------------------------------------------ %
\subsection*{Question 11. (20 marks) }

The frequency of 0 as an input to a binary channel is 0.6. If O is the
input, then 0 is the output with probability 0.8. If l is the input, then l
is the output with probability 0.9.
\begin{itemize}
	\item[a.](4 marks) Calculate the information per bit contained in the input.
	\item[b.](2 marks)Calculate the probability that the output is 0.
	\item[c.](2 marks) Calculate the probability that the output is l,
	\item[d.](2 marks) Calculate the probability that the input is 0 given that the
	output is O.
	\item[e.](2 marks) Calculate the probability that the input is l given that
	the output is 1,
	\item[f.](2 marks) Calculate the probability that the input is l given that
	the output is O.
	\item[g.](2 marks) Calculate the probability that the input is 0 given that
	the output is l.
	\item[h.](6 marks) Calculate the amount of information transmitted by the channel.
	\item[i.](3 marks) Derive the globally optimal reconstruction rule.
\end{itemize}
\section{Information Theory}

\subsection*{Question 61. (27 marks) }
A source language has 5 symbols A, B, C, D and E. The associated
probabilities of these symbols are given in the table below:
\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		Symbol & Probability \\ \hline
		A & 0.60 \\
		B & 0.30 \\
		C & 0.05 \\
		D & 0.03 \\
		E & 0.02 \\
		\hline
	\end{tabular}
\end{center}

\begin{itemize}
	\item[a.](5 marks) Calculate the entropy of the source language.
	\item[b.](10 marks) Define a Huffman binary code for the source language.
	\item[c.](3 marks) Calculate the efficiency of the code in (b) above.
	\item[d.](2 marks) Calculate the redundancy of the code in (b) above.
	\item[e.](2 marks) Briefly state what is meant by the Prefix Condition.
\end{itemize}

\subsection*{Question 62. (3 marks) } For each of the following codes state whether they are
\begin{itemize}
	\item[a.] (1 mark) nonsingular
	\item[b.] (1 mark) uniquely decodable
	\item[c.] (1 mark) instantaneous
\end{itemize}
\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
		& Code 1 & Code 2 & Code 3 \\
		A & 1 & 10 & 1 \\
		B & 01 & 01 & 00 \\
		C & 001 & 10 & 000 \\
		\hline
	\end{tabular}
\end{center}

\subsection*{Question 63. (10 marks) } Consider a data source drawn from an alphabet (A, B, C, D) with probability
distribution (0.3, 0.4, 0.2, 0.1).
\begin{itemize}
	\item[a.] Derive a fixed length binary code and a Huffman code for the source.
	\item[b.] Comment on the difference in shape of a binary tree representing the fixed
	length code and the Huffman tree for the source.
	\item[c.] Discuss which code is closer to the optimal. Show all your work to justify your
	answer.
\end{itemize}



%------------------------------------------------------------ %
\subsection*{Question 6}

The input source to a noisy communication channel is a random variable X over three symbols a,b,c. The output from this channel is a random variable Y over the same three symbols. The joint distribution of these two random variables is as follows:


\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline	 &x=a	&x=b&	     x=c\\ \hline
y=a	 &0.25	&0	 &    0.125 \\ \hline
y=b	 &0	    &0.125&	 0 \\ \hline
y=c	 &0.125&	0.25&	 0.125 \\ \hline
\end{tabular} 
\end{center}

\begin{itemize}
\item Write down the marginal distributions for X and Y. %(Last Week)
\item Compute the marginal entropies $H(X)$ and $H(Y)$ %(Last Week)
\item Compute the joint entropy $H(X,Y)$ of the two random variables. % (Last Week)
\item
Compute the mutual information $I(X;Y)$.
\item 
Compute the conditional entropies $H(X|Y)$ and $H(Y|X)$.
\item 
From Formulae: \[I(X,Y) = H(X) — H(X|Y)\]
\end{itemize}
%------------------------------------------------------------ %
\subsection*{Question 7}

The frequency of 0 as an input to a binary channel is 0.6. If 0 is the input, then 0 is the output with probability 0.8. If l is the input, then l is the output with probability 0.9.

Write out the channel transition matrix

\begin{itemize}
\item	Calculate the output probabilities [P(Y)] 
\item	Compute the joint probabilities [P(X,Y)]
\item	Calculate the probability that the input is 0 given that the output is O. 
\item Calculate the probability that the input is l given that the output is 1, 
\item	Calculate the probability that the input is l given that the output is O.
\item	Calculate the probability that the input is 0 given that the output is l. 
\end{itemize}
%------------------------------------------------------------ %

%------------------------------------------------------------ %
%\section*{Question 4}

%-----------------------------------------%
\subsection*{Question 8 }
Consider a DMS X with symbols $\{x_l,x_2,x_3,x_4\}$. The table below lists four possible
binary codes.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{images5/12ACodes}
\caption{}
\label{fig:12ACodes}
\end{figure}



%IMAGE OF KRAFT INEQUALITY HERE
\begin{itemize}
\item[(i)] Show that all codes except B satisfy the Kraft inequality (formula below)
m
\item[(ii)] Show that codesA and D are uniquely decodable but code B and C are not
uniquely demdable.
\end{itemize}
\begin{figure}[h!]
\centering
\includegraphics[width=0.4\linewidth]{images/12AKraftIneq}
\caption{}
\label{fig:12AKraftIneq}
\end{figure}


%-----------------------------------------%
\subsection*{Question 9}
A DMS X has live symbols $\{x_l,x_2,x_3,x_4,x_5\}$ with $P(x_1) = 0.2$, $P(x_2)=0.15$, $P(x_3) =0.05$,
$P(x_4) = 0.10$, and $P(x_5) = 0.5$. 
\begin{itemize}
\item[(a)] Construct a Shann0n—Fano code for X, and calculate the efficiency of the code.
\item[(b)] Repeat for the Huffman code and compare the results.
\end{itemize}

 

\section*{Question 1}

A input source is a random variable X with a four letter alphabet $\{A,B,C,D\}$.
There are four different probability distributions presented below. 
Compute the entropy for each case.

\begin{tabular}{|cc|c|c|c|}
\hline
	&	Case 1	&	Case 2	&	Case 3	&	Case 4	\\	
Xi	&	P(Xi)	&	P(Xi)	&	P(Xi)	&	P(Xi)	\\	\hline
A	&	0.25	&	0.25	&	0.7	&	0.97	\\	\hline
B	&	0.25	&	0.5	&	0.1	&	0.01	\\	\hline
C	&	0.25	&	0.125	&	0.1	&	0.01	\\	\hline
D	&	0.25	&	0.125	&	0.1	&	0.01	\\	\hline
\end{tabular} 


\section{Question 2}

The input source to a noisy communication channel is a random variable X over three symbols a,b,c. The output from this channel is a random variable Y over the same three symbols. The joint distribution of the these two random variables is as follows:

\begin{tabular}{|c|c|c|c|}
\hline	&	y=a	&	y=b	&	y=c	\\	\hline
x=a	&	0.25	&	0	&	0.125	\\	\hline
x=b	&	0	&	0.125	&	0.25	\\	\hline
x=c	&	0.125	&	0	&	0.125	\\	\hline
\end{tabular} 


\begin{itemize}
\item Write down the marginal distributions for X and Y.

\item Compute the marginal entropies $H(X)$ and $H(Y)$

\item Compute the joint entropy $H(X,Y)$ of the two random variables.
\end{itemize}
%---------------------------------------------------- %
\section*{Question 3a}
A four letter alphabet is encoded into binary form according to
\begin{tabular}{|c|c|c|c|c|}
\hline
Case 1	&	A:  10 	&	C:  110	&	G:  111	& T:  0 \\ \hline
Case 2	&	A:  00	&	C:  01	&		G: 10	&	T: 11 \\ \hline
\end{tabular} 
	

Using the code presented in case 1, decode the following sequence:	
\[11110001011010\]
Encode this message using the code from case 2. Compare the length of messages in both cases.

\section*{Question 3a}
Given that the alphabet has the following distribution 
\begin{tabular}{|c|c|c|c|c|}
\hline
Xi & A & C & G & T \\ \hline
P(Xi) & 0.25 & 0.125 & 0.125 & 0. 5 \\
\hline
\end{tabular} 

Compute the average symbol length for both cases.



\section{KB Tutorial 12}


\subsection*{Question 1}

A file contains the following characters:\\[-0.1cm]
\begin{center}
	\begin{tabular}{|c|ccccc|}
		\hline
		&&&&& \\[-0.4cm]
		$x_i$     & a & b & c & d & e \\[0.1cm]
		\hline
		&&&&& \\[-0.4cm]
		$p(x_i)$  & 0.5 & 0.15 & 0.2 & 0.05 & 0.1 \\[0.1cm]
		\hline
		\multicolumn{6}{c}{}\\[-0.2cm]
	\end{tabular}
\end{center}

{\bf(a)} Calculate the entropy for this file. \quad {\bf(b)} Construct a Huffman code. \quad \\{\bf(c)} Calculate the expected length of the code. \quad {\bf(d)} Calculate the efficiency of the code.



\subsection*{Question 2}

A file contains the following characters:\\[-0.1cm]
\begin{center}
	\begin{tabular}{|c|cccccc|}
		\hline
		&&&&&& \\[-0.4cm]
		$x_i$     & a & b & c & d & e & f\\[0.1cm]
		\hline
		&&&&&& \\[-0.4cm]
		$p(x_i)$  & 0.3 & 0.25 & 0.2 & 0.12 & 0.08 & 0.05 \\[0.1cm]
		\hline
		\multicolumn{7}{c}{}\\[-0.2cm]
	\end{tabular}
\end{center}

{\bf(a)} Calculate the entropy for this file. \quad {\bf(b)} Construct a Huffman code. \quad \\{\bf(c)} Calculate the expected length of the code. \quad {\bf(d)} Calculate the efficiency of the code.


\subsection*{Question 3}

A file contains the following characters:\\[-0.1cm]
\begin{center}
	\begin{tabular}{|c|ccccccc|}
		\hline
		&&&&&&& \\[-0.4cm]
		$x_i$     & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ \\[0.1cm]
		\hline
		&&&&&&& \\[-0.4cm]
		$p(x_i)$  & 0.3 & 0.2 & 0.2 & 0.1 & 0.1 & 0.05 & 0.05 \\[0.1cm]
		\hline
		\multicolumn{7}{c}{}\\[-0.2cm]
	\end{tabular}
\end{center}

{\bf(a)} Calculate the entropy for this file. \quad {\bf(b)} Construct a Huffman code. \quad \\{\bf(c)} Calculate the expected length of the code. \quad {\bf(d)} Calculate the efficiency of the code.




\subsection*{Question 4}

A file contains the following characters:\\[-0.1cm]
\begin{center}
	\begin{tabular}{|c|ccccccccccc|}
		\hline
		&&&&&&&&&&& \\[-0.4cm]
		$x_i$     & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$  & $x_8$ & $x_9$  & $x_{10}$ & $x_{11}$ \\[0.1cm]
		\hline
		&&&&&&&&&&& \\[-0.4cm]
		$p(x_i)$  & 0.3 & 0.15 & 0.15 & 0.1 & 0.05 & 0.05 & 0.05 & 0.05 & 0.04 & 0.04 & 0.02 \\[0.1cm]
		\hline
		\multicolumn{12}{c}{}\\[-0.2cm]
	\end{tabular}
\end{center}

{\bf(a)} Construct a Huffman code.
\newpage
%----------------------------------------------------------------------------- %
\section*{Question 3b}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
	&	A	&	C	&	G	&	T	&		\\ \hline	
Code 1	&	10	&	110	&	111	&	0	&		\\ \hline	
$p(x_i)$	&	0.25	&	0.125	&	0.125	&	0.5	&		\\ \hline	
$n(x_i)$	&	2	&	3	&	3	&	1	&		\\ \hline	
$p(x_i) \times n(x_i)$	&	0.5	&	0.375	&	0.375	&	0.5	&	\textbf{1.75}	\\ \hline	
	&		&		&		&		&		\\ \hline	\hline
Code 2	&	00	&	01	&	10	&	11	&		\\ \hline	
$p(x_i)$	&	0.25	&	0.125	&	0.125	&	0.5	&		\\ \hline	
$n(x_i)$	&	2	&	2	&	2	&	2	&		\\ \hline	
$p(x_i) \times n(x_i)$	&	0.5	&	0.25	&	0.25	&	1	&	\textbf{2}	\\ \hline	
\end{tabular} 
\end{center}

Expected Code Lengths $E(L)$ for Codes 1 and 2 are 1.75b and 2b respectively.

%----------------------------------------------------------------------------- %
\section*{Question 5}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline	&		&	Shannon Fano	&	Huffman	&		&		&		&	\\	
	&		&	 Code	&	Code	&	$n(x_i)$	&	$p(x_i) \times n(x_i)$	&	$I(x_i)$	&	$p(x_i) \times I(x_i)$	\\ \hline
$x_5$	&	0.50	&		&		&	1	&	0.50	&	1	&	0.5000	\\ \hline
$x_1$	&	0.20	&		&		&	2	&	0.40	&	2.3219	&	0.4644	\\ \hline
$x_2$	&	0.15	&		&		&	3	&	0.45	&	2.7370	&	0.4105	\\ \hline
$x_4$	&	0.10	&		&		&	4	&	0.40	&	3.3219	&	0.3322	\\ \hline
$x_3$	&	0.05	&		&		&	4	&	0.20	&	4.3219	&	0.2161	\\ \hline
	&		&		&		&		&	1.95	&		&	1.9232	\\ \hline
 
\end{tabular} 
\end{center}

\end{document}


\end{document}
