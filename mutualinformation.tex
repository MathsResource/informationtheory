

%---------------------------------------------------------------------------------------------------------------------------------------%

\section{ MUTUAL INFORMATION}
\textbf{A. Conditional and Joint Entropies:}\\
Using the input probabilities P(x,), output probabilities $P(y_i)$, transition probabilities P(yJ|>r,),
and joint probabilities P(x,, yy), we can define the following various entropy functions for a channel
with m inputs and n outputs:
 
\begin{itemize}
\item H(X) = - X P(xi) log %; P(x;) (10.21)
\item H(Y) = -2P(yj)]%<>gz Pty;) UU-22)
\item H<X I Y) = - X X %P<>¤r,yy)1<¤gz Ptmlyj) <10.23)
\item H = -2 ZP%(>¢..y,) 1022 P(y,|X.) (10-24)
\item H(X, Y) - -2 Z F%<><..y,)1¤zz P(><r.y;) <10·25)
\end{itemize}


%---------------------------------------------------------------------------------------------------------------------------------------%

\section{Conditional and Joint Entropy}
These entropies can be interpreted as follows: H(X) is the average uncertainty of the channel input,
and H(Y) is the average uncertainty of the channel output. The conditional entropy H(X]Y) is a
measure of the average uncertainty remaining about the channel input after the channel output has
been observed. And H(X] Y) is sometimes called the equivncation of X with respect to Y. \begin{itemize} \item The
conditional entropy H(Y|X) is the average uncertainty of the channel output given that X was
transmitted.\item  The joint entropy H(X, Y) is the average uncertainty of the communication channel as a
whole.\end{itemize}

%-----------------------------------------------------------------------------------------------%

Two useful relationships among the above various entropies are
\begin{itemize} \item
$H(X, Y)=H(X|Y)+H(Y) (10,26)$
$H(X,Y)=H(Y|X)+H(X) (10.27)$
\end{itemize}
B. Mutual Information:
The mutual information 1(X; Y) of a channel is deiined by
I(X; Y) = H(X)— H(X|Y) b/symbol (10.28)
