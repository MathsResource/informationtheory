{Information content of a Discrete Memoryless Source}
\begin{itemize}
\item The amount of information contained in an event is closely related to its uncertainty.
\item Messages containing knowledge of high probability of occurrence convey relatively little information.

\item We note that if an event is certain (that is, the event occurs with probability of 1), then we can say that it conveys zero \textit{information}.

\item Conversely - very unlikely events are ``high information" events ( e.g. Alarms).
\end{itemize}

Thus, a mathematical measure of information should be a function of the probability of the outcome and should satisfy the following axioms:
\begin{itemize}
\item[1.] Information should be proportional to the uncertainty of an outcome.
\item[2.] Information contained in independent outcomes should add (see axioms).
\end{itemize}


%----------------------------------------------------------------------------------------------------------%

{Information Content of a Symbol}

\begin{itemize}
\item Consider a DMS, denoted by X, with alphabet ${x,.x2. ...,x_n}$.
\item The information content of a symbol
$x_l$, denoted by $I(x_i)$, is defined by

\[  I(x_i)  = \mbox{log}_b\left( \frac{1}{ P(x_i)}\right) =  -\mbox{log}_b[ P(x_i) ]  \]


where $P(x_i)$ is the probability of occurrence of symbol $x_i$. \item ( We will discuss what $b$ is shortly.)
\end{itemize}



%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%
