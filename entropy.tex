

%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%

{Entropy}
\begin{itemize}
\item The quantity $H(X)$ is called the \emph{\textbf{entropy}} of source $X$. \item It is a measure of the average information content per random symbol.
\item The source entropy $H(X)$ can be considered as the average amount of uncertainty
within source $X$ that is resolved by use of the alphabet.

\item Note that for if  binary source X that generates independent symbols $0$ and $1$ with equal probability, the source entropy $H(X)$ is
\[ H(X ) = -1/2 \mbox{log}_2 (1/2) - 1/2 \mbox{log}_2 (1/2) \mbox{   b/symbol}  \]
%(Remark :$\mbox{log}_2 ({1\over 2}) = -1$).
\end{itemize}


\begin{itemize}
\item The source entropy$ H(X)$ satisfies the following relation:
\[0 \leq H(X) \leq \mbox{log}_2(m) \]where $m$ is the size (number of symbols) of the alphabet of source X ).
\item  The lower bound corresponds to no uncertainty, which occurs when one symbol has probability $P(x_i) = l$ (i.e. X emits the same symbol all the time.
\item The upper bound corresponds to the maximum uncertainty which occurs when $P(x_i) = 1 /m$ for all $i$. that is, when all symbols are equally likely to be emitted by X.
 \end{itemize}


%-----------------------------------------------------------------------------------------------------------------------------------------------------------%


{Entropy: Example}
A DMS $X$ has four symbols $x_1 , x_2, x_3, x_4$ with probabilities $P(x_1) = 0.4, P(x_2) = 0.3. P(x_3) = 0.2.
P(x_4) = 0.1$.
\begin{itemize}
\item[(a)] Calculate $H(X)$.
\item[(b)] Find the amount of information contained in the messages $x_lx_2x_lx_3$ and $x_4x_3x_3x_2$.
\end{itemize}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------%


{Entropy: Example part a}

\[ H(X) = - \sum \limits^{4}_{i=1} P(x_i) log_2 [P(x_i)] \]

\[ H(X) = -0.4\mbox{log}_2(0.4) - 0.3\mbox{log}_2(0.3)  -0.2\mbox{log}_2(0.2)  -0.1\mbox{log}_2(0.14) \]



\[ H(X) =  0.5288 + 0.5210 + 0.4644 + 0.3322  = \textbf{1.85} \mbox{b/sec} \]




%-----------------------------------------------------------------------------------------------------------------------------------------------------------%


{Entropy: Example part b}
\begin{itemize}
\item (Remark: from probability, recall independent events) \bigskip
\item $P(x_lx_2x_lx_3) = 0.4\times 0.30 \times 0.40 \times 0.20  = 0.0096$ \bigskip
\item $I(x_lx_2x_lx_3) = -\mbox{log}_2(0.0096)  = 6.70$b/symbol \bigskip
\end{itemize}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------%


{Entropy: Example part c}
\begin{itemize}
\item $P(x_4x_3x_3x_2) = 0.1\times 0.20 \times 0.20 \times 0.30  = 0.0012$ \bigskip
\item $I(x_lx_2x_lx_3) = -\mbox{log}_2(0.0012)  = 9.70$b/symbol \bigskip
\end{itemize}



%-----------------------------------------------------------------------------------------------------------------------------------------------------------%


\section{Information Rate}
If the time rate at which source X emits symbols is $r$ (symbols/second), the information rate R of the
source is given by

\[R = rH(X) \mbox{      (b/second)} \]




%-----------------------------------------------------------------------------------------------------------------------------------------------------------%

{Information Rate : Example}
\begin{itemize}

\item A high-resolution TV picture consists of about $2 \times 10^6$ picture elements (symbols) and 16
different brightness levels. \item Pictures are repeated at a rate of 32 per second. \item All picture elements
are assumed to be independent, and all levels have equal likelihood of occurrence. \item Calculate the
average rate of information conveyed by this TV picture source.

\end{itemize}



\begin{itemize}
\item $H(X) = - \sum \limits^{16}_{i=1} \frac{1}{16} \mbox{log}_2 \frac{1}{16}$ \bigskip

\item i.e. $H(X) = [ -\frac{1}{16} \mbox{log}_2\frac{1}{16} ] + [- \frac{1}{16} \mbox{log}_2\frac{1}{16} ] \ldots [ -\frac{1}{16} \mbox{log}_2\frac{1}{16}) ] $ \bigskip
\item Sixteen identical terms. Compute one and multiply by 16.

\[ H(X) = 16 \times [ -\frac{1}{16} \mbox{log}_2\frac{1}{16} ]  = -\mbox{log}_2\frac{1}{16} = -(-4) = 4\] \bigskip
\item $H(X)= 4$ b
\item $r =  2(10^6)(32) = 64(10^6)$ elements/sec \bigskip

\item $R = rH(X) = 64(10^6)(4) = 256(l0^6) \mbox{ b/sec } = 256 \mbox{ Mb/sec }$ \bigskip
\end{itemize}
